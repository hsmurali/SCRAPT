{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Cluster_Evaluation_Utils import *\n",
    "from sys import exit\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Prepare_Data_All_Sampling_Rates(Sample_dir, sim_cutoff):\n",
    "    experiment_variations = listdir(Sample_dir)\n",
    "    out = {}\n",
    "    for v in experiment_variations:\n",
    "        print(v)\n",
    "        if v.startswith(\"CDHIT\"): # and \"16_Threads\" in v:\n",
    "            #cdhit = pd.read_csv(Sample_dir+v+'/cd_hit_'+str(int(sim_cutoff*100))+'.txt', sep = ';', \n",
    "            #                    names = ['Clusters','Density'])\n",
    "            cdhit = Parse_CDHIT_Outputs(Sample_dir+v+\"/CDHIT_EST_\"+str(sim_cutoff)+\"_g_1/CDHIT_EST.\"+\n",
    "                                        str(sim_cutoff)+\".clusters.clstr\")\n",
    "            x,y = Compute_Fragmentation_Measure(cdhit, 1)\n",
    "            out['CDHIT'] = {'Fragmentation_Measure':(x,y), 'Clusters':cdhit}\n",
    "        \n",
    "        elif v.startswith(\"UCLUST\"):\n",
    "            uclust = pd.read_csv(Sample_dir+v+'/UCLUST_'+str(int(sim_cutoff*100))+'.txt', sep = ';', \n",
    "                                names = ['Clusters','Density'])\n",
    "            x,y = Compute_Fragmentation_Measure(uclust['Density'].tolist(), 1)\n",
    "            out['UCLUST'] = {'Fragmentation_Measure':(x,y), 'Clusters':uclust['Density'].tolist()}\n",
    "        \n",
    "        elif v.startswith(\"DNACLUST\"):\n",
    "            dnaclust_cluster_sizes = Parse_DNACLUST_Outputs(Sample_dir+v+'/dnaclust_'+str(sim_cutoff)+'.txt')\n",
    "            x,y=Compute_Fragmentation_Measure(dnaclust_cluster_sizes, 1)\n",
    "            out['DNACLUST'] = {'Fragmentation_Measure':(x,y), 'Clusters':dnaclust_cluster_sizes}\n",
    "        \n",
    "        elif v.startswith('Parameterize'):\n",
    "            try:\n",
    "                data_dir = Sample_dir+v+'/sim_'+str(sim_cutoff)+'/'\n",
    "                alphas = listdir(data_dir)\n",
    "                for a in alphas:\n",
    "                    if a.startswith(\"alpha_\"):\n",
    "                        deltas = listdir(data_dir+a+'/')\n",
    "                        for d in deltas:\n",
    "                            if d.startswith(\"delta\"):\n",
    "                                print(d)\n",
    "                                filedir = data_dir+a+'/'+d+'/'\n",
    "                                df_clusters, df_SCRAPT_summary = Compile_Cluster_Summaries(filedir)\n",
    "                                cluster_list = df_clusters['Density'].tolist()\n",
    "                                x,y=Compute_Fragmentation_Measure(cluster_list)\n",
    "\n",
    "                                try:\n",
    "                                    out[float(a.replace(\"alpha_\",\"\"))][d] = {'Cluster_Summary':df_SCRAPT_summary, \n",
    "                                                                             'Clusters':df_clusters, \n",
    "                                                                             'Fragmentation_Measure':(x,y)} \n",
    "                                except KeyError:\n",
    "                                    out[float(a.replace(\"alpha_\",\"\"))] = {d:{'Cluster_Summary':df_SCRAPT_summary, \n",
    "                                                                             'Clusters':df_clusters, \n",
    "                                                                             'Fragmentation_Measure':(x,y)}}\n",
    "            except FileNotFoundError:\n",
    "                pass    \n",
    "        elif v.startswith('Fixed') or v.startswith('Adaptive'):\n",
    "                data_dir = Sample_dir+v+'/sim_'+str(sim_cutoff)+'/'\n",
    "                alphas = listdir(data_dir)\n",
    "                for a in alphas:\n",
    "                    if a.startswith(\"alpha_\"):\n",
    "                        print(a)\n",
    "                        filedir = data_dir+a+'/'\n",
    "                        df_clusters, df_SCRAPT_summary = Compile_Cluster_Summaries(filedir)\n",
    "                        cluster_list = df_clusters['Density'].tolist()\n",
    "                        x,y=Compute_Fragmentation_Measure(cluster_list)\n",
    "                        try:\n",
    "                            out[float(a.replace(\"alpha_\",\"\"))][v] = {'Cluster_Summary':df_SCRAPT_summary, \n",
    "                                                                     'Clusters':df_clusters, \n",
    "                                                                     'Fragmentation_Measure':(x,y)} \n",
    "                        except KeyError:\n",
    "                            out[float(a.replace(\"alpha_\",\"\"))] = {v:{'Cluster_Summary':df_SCRAPT_summary, \n",
    "                                                                     'Clusters':df_clusters, \n",
    "                                                                     'Fragmentation_Measure':(x,y)}}\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/Users/harihara/Mount-2/Experiments/Tara_Oceans_Polar/'\n",
    "outdir = '/Users/harihara/Research-Activities/Data/SCRAPT/'\n",
    "if not isdir(outdir):\n",
    "    mkdir(outdir)\n",
    "if not isdir(outdir+'Tara_Oceans_Polar/'):\n",
    "    mkdir(outdir+'Tara_Oceans_Polar/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DADA2_Benchmarks\n",
      "._.DS_Store\n",
      ".DS_Store\n",
      "Fixed_With_Modeshifting\n",
      "alpha_0.5\n",
      "alpha_0.05\n",
      "alpha_0.01\n",
      "alpha_0.1\n",
      "Adaptive_Without_Modeshifting\n",
      "alpha_0.5\n",
      "alpha_0.05\n",
      "alpha_0.01\n",
      "alpha_0.1\n",
      "Fixed_Without_Modeshifting\n",
      "alpha_0.5\n",
      "alpha_0.05\n",
      "alpha_0.01\n",
      "alpha_0.1\n",
      "Adaptive_With_Modeshifting\n",
      "alpha_0.5\n",
      "alpha_0.05\n",
      "alpha_0.01\n",
      "alpha_0.1\n"
     ]
    }
   ],
   "source": [
    "cluster_op_99 = Prepare_Data_All_Sampling_Rates(data_dir, 0.99)\n",
    "pickle.dump(cluster_op_99, open(outdir+\"Tara_Oceans_Polar/sim_0.99.pkl\",\"wb\"))\n",
    "del cluster_op_99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DADA2_Benchmarks\n",
      "._.DS_Store\n",
      ".DS_Store\n",
      "Fixed_With_Modeshifting\n",
      "alpha_0.5\n",
      "alpha_0.05\n",
      "alpha_0.01\n",
      "alpha_0.1\n",
      "Adaptive_Without_Modeshifting\n",
      "alpha_0.5\n",
      "alpha_0.05\n",
      "alpha_0.01\n",
      "alpha_0.1\n",
      "Fixed_Without_Modeshifting\n",
      "alpha_0.5\n",
      "alpha_0.05\n",
      "alpha_0.01\n",
      "alpha_0.1\n",
      "Adaptive_With_Modeshifting\n",
      "alpha_0.5\n",
      "alpha_0.05\n",
      "alpha_0.01\n",
      "alpha_0.1\n"
     ]
    }
   ],
   "source": [
    "cluster_op_98 = Prepare_Data_All_Sampling_Rates(data_dir, 0.98)\n",
    "pickle.dump(cluster_op_98, open(outdir+\"Tara_Oceans_Polar/sim_0.98.pkl\",\"wb\"))\n",
    "del cluster_op_98"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DADA2_Benchmarks\n",
      "._.DS_Store\n",
      ".DS_Store\n",
      "Fixed_With_Modeshifting\n",
      "alpha_0.5\n",
      "alpha_0.05\n",
      "alpha_0.01\n",
      "alpha_0.1\n",
      "Adaptive_Without_Modeshifting\n",
      "alpha_0.5\n",
      "alpha_0.05\n",
      "alpha_0.01\n",
      "alpha_0.1\n",
      "Fixed_Without_Modeshifting\n",
      "alpha_0.5\n",
      "alpha_0.05\n",
      "alpha_0.01\n",
      "alpha_0.1\n",
      "Adaptive_With_Modeshifting\n",
      "alpha_0.5\n",
      "alpha_0.05\n",
      "alpha_0.01\n",
      "alpha_0.1\n"
     ]
    }
   ],
   "source": [
    "cluster_op_97 = Prepare_Data_All_Sampling_Rates(data_dir, 0.97)\n",
    "pickle.dump(cluster_op_97, open(outdir+\"Tara_Oceans_Polar/sim_0.97.pkl\",\"wb\"))\n",
    "del cluster_op_97"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DADA2_Benchmarks\n",
      "._.DS_Store\n",
      ".DS_Store\n",
      "Fixed_With_Modeshifting\n",
      "alpha_0.5\n",
      "alpha_0.05\n",
      "alpha_0.01\n",
      "alpha_0.1\n",
      "Adaptive_Without_Modeshifting\n",
      "alpha_0.5\n",
      "alpha_0.05\n",
      "alpha_0.01\n",
      "alpha_0.1\n",
      "Fixed_Without_Modeshifting\n",
      "alpha_0.5\n",
      "alpha_0.05\n",
      "alpha_0.01\n",
      "alpha_0.1\n",
      "Adaptive_With_Modeshifting\n",
      "alpha_0.5\n",
      "alpha_0.05\n",
      "alpha_0.01\n",
      "alpha_0.1\n"
     ]
    }
   ],
   "source": [
    "cluster_op_96 = Prepare_Data_All_Sampling_Rates(data_dir, 0.96)\n",
    "pickle.dump(cluster_op_96, open(outdir+\"Tara_Oceans_Polar/sim_0.96.pkl\",\"wb\"))\n",
    "del cluster_op_96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DADA2_Benchmarks\n",
      "._.DS_Store\n",
      ".DS_Store\n",
      "Fixed_With_Modeshifting\n",
      "alpha_0.5\n",
      "alpha_0.05\n",
      "alpha_0.01\n",
      "alpha_0.1\n",
      "Adaptive_Without_Modeshifting\n",
      "alpha_0.5\n",
      "alpha_0.05\n",
      "alpha_0.01\n",
      "alpha_0.1\n",
      "Fixed_Without_Modeshifting\n",
      "alpha_0.5\n",
      "alpha_0.05\n",
      "alpha_0.01\n",
      "alpha_0.1\n",
      "Adaptive_With_Modeshifting\n",
      "alpha_0.5\n",
      "alpha_0.05\n",
      "alpha_0.01\n",
      "alpha_0.1\n"
     ]
    }
   ],
   "source": [
    "cluster_op_95 = Prepare_Data_All_Sampling_Rates(data_dir, 0.95)\n",
    "pickle.dump(cluster_op_95, open(outdir+\"Tara_Oceans_Polar/sim_0.95.pkl\",\"wb\"))\n",
    "del cluster_op_95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
